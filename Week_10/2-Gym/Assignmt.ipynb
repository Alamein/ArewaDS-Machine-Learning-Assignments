{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\",render_mode = \"human\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\",render_mode = \"human\")\n",
    "observation, info,__ = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "   observation, reward, terminated, truncated, info,_ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"{observation} -> {reward}\")\n",
    "   \n",
    "env.close()\n",
    "\n",
    "\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "\n",
    "\n",
    "def discretize(x):\n",
    "    return tuple(((x - np.array([-1.2, -0.07])) / np.array([0.1, 0.01])).astype(int))\n",
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(discretize(observation))\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "Q = {}\n",
    "actions = (0,1,2)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]\n",
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "for epoch in range(1000):\n",
    "    observation, info = env.reset()\n",
    "    cum_reward=0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    # == do the simulation ==\n",
    "    while not terminated and not truncated:\n",
    "        s = discretize(observation)\n",
    "        if random.random()<epsilon:\n",
    "        # exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions,weights=v)[0]\n",
    "        else:\n",
    "        # exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(a)\n",
    "        cum_reward+=reward\n",
    "        ns = discretize(observation)\n",
    "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (reward + gamma * max(qvalues(ns)))\n",
    "    \n",
    "                \n",
    "    cum_rewards.append(cum_reward)\n",
    "    rewards.append(cum_reward)\n",
    "    # == Periodically print results and calculate average reward ==\n",
    "    if epoch%50==0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards=[]\n",
    "plt.plot(rewards)\n",
    "def running_average(x,window):\n",
    "    return np.convolve(x,np.ones(window)/window,mode='valid')\n",
    "\n",
    "plt.plot(running_average(rewards,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not terminated and not truncated:\n",
    "    s = discretize(observation)\n",
    "    v = probs(np.array(qvalues(s)))\n",
    "    a = random.choices(actions,weights=v)[0]\n",
    "    observation, reward, terminated, truncated, info = env.step(a)\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "env = gym.make(\"MountainCar-v0\", render_mode = \"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "i=0\n",
    "ims = []\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not terminated and not truncated:\n",
    "   s = discretize(observation)\n",
    "   img= env.render()\n",
    "   ims.append(Image.fromarray(img))\n",
    "   v = probs(np.array([Q.get((s,a),0) for a in actions]))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   observation, reward, terminated, truncated, info = env.step(a)\n",
    "   i+=1\n",
    "   \n",
    "env.close()\n",
    "ims[0].save('images/mountain-car.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e880a7f5f27ad91acd9cec33fbf0d93d234c7ab08dff5f8487ebbcd75c6e380"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
